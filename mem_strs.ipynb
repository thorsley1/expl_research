{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fancy_einsum einops jaxtyping wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from fancy_einsum import einsum\n",
    "import einops\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "t.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 16\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 1024\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024\n",
    "    d_head: int = 8\n",
    "    d_mlp: int = 64\n",
    "    n_heads: int = 2\n",
    "    n_layers: int = 1\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(t.nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = t.nn.Parameter(t.ones(cfg.d_model))\n",
    "        self.b = t.nn.Parameter(t.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(self, residual):\n",
    "        residual_std = (residual.var(dim=-1, keepdim=True, unbiased=False) + self.cfg.layer_norm_eps).sqrt()\n",
    "        residual = (residual - residual.mean(dim=-1, keepdim=True)) / residual_std\n",
    "        return residual * self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(t.nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = t.nn.Parameter(t.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        t.nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.W_E[tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbed(t.nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = t.nn.Parameter(t.empty((cfg.n_ctx, cfg.d_model)))\n",
    "        t.nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        batch, seq_len = tokens.shape\n",
    "        return einops.repeat(self.W_pos[:seq_len], \"seq d_m -> b seq d_m\", b=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(t.nn.Module):\n",
    "\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = t.nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_K = t.nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_V = t.nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_O = t.nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        self.b_Q = t.nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_K = t.nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_V = t.nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_O = t.nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        t.nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        t.nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        t.nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        t.nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, normalized_resid_pre):\n",
    "\n",
    "        q = einsum(\"b p d_m, h d_m d_h -> b p h d_h\", normalized_resid_pre, self.W_Q) + self.b_Q\n",
    "        k = einsum(\"b p d_m, h d_m d_h -> b p h d_h\", normalized_resid_pre, self.W_K) + self.b_K\n",
    "        v = einsum(\"b p d_m, h d_m d_h -> b p h d_h\", normalized_resid_pre, self.W_V) + self.b_V\n",
    "\n",
    "        attn_scores = einsum(\"b p_Q h d_h, b p_K h d_h -> b h p_Q p_K\", q, k)\n",
    "\n",
    "        attn_scores_masked = self.apply_causal_mask(attn_scores / self.cfg.d_head ** 0.5)\n",
    "        attn_pattern = attn_scores_masked.softmax(-1)\n",
    "\n",
    "        z = einsum(\"b p_K h d_h, b h p_Q p_K -> b p_Q h d_h\", v, attn_pattern)\n",
    "\n",
    "        attn_out = einsum(\"b p_Q h d_h, h d_h d_m -> b p_Q d_m\", z, self.W_O) + self.b_O\n",
    "        return attn_out\n",
    "\n",
    "    def apply_causal_mask(self, attn_scores):\n",
    "\n",
    "        all_ones = t.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device)\n",
    "        mask = t.triu(all_ones, diagonal=1).bool()\n",
    "        attn_scores.masked_fill_(mask, -1e5)\n",
    "        return attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(t.nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = t.nn.Parameter(t.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        self.W_out = t.nn.Parameter(t.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        self.b_in = t.nn.Parameter(t.zeros((cfg.d_mlp)))\n",
    "        self.b_out = t.nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        t.nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        t.nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, normalized_resid_mid):\n",
    "\n",
    "        pre = einsum(\"b p d_m, d_m d_mlp -> b p d_mlp\", normalized_resid_mid, self.W_in) + self.b_in\n",
    "        post = t.nn.GELU()(pre)\n",
    "        mlp_out = einsum(\"b p d_mlp, d_mlp d_m -> b p d_m\", post, self.W_out) + self.b_out\n",
    "        return mlp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(t.nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(self, resid_pre):\n",
    "        resid_mid = resid_pre + self.attn(self.ln1(resid_pre))\n",
    "        resid_post = resid_mid + self.mlp(self.ln2(resid_mid))\n",
    "        return resid_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unembed(t.nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = t.nn.Parameter(t.empty((cfg.d_model, cfg.d_vocab)))\n",
    "        t.nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
    "        self.b_U = t.nn.Parameter(t.zeros((cfg.d_vocab), requires_grad=False))\n",
    "\n",
    "    def forward(self, normalized_resid_final):\n",
    "        return einsum(\"b p d_m, d_m d_v -> b p d_v\", normalized_resid_final, self.W_U) + self.b_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(t.nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = t.nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        residual = self.embed(tokens) + self.pos_embed(tokens)\n",
    "        for block in self.blocks:\n",
    "            residual = block(residual)\n",
    "        logits = self.unembed(self.ln_final(residual))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memorised strings training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_vocab = 128\n",
    "\n",
    "model_cfg = Config(\n",
    "    d_model=32,\n",
    "    n_heads=4,\n",
    "    d_head=8,\n",
    "    d_mlp=128,\n",
    "    n_layers=1,\n",
    "    n_ctx=128,\n",
    "    d_vocab=d_vocab\n",
    ")\n",
    "\n",
    "model = Transformer(model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerTrainingArgs():\n",
    "\tbatch_size = 512\n",
    "\tepochs = 10\n",
    "\tnum_steps = 1000\n",
    "\tlr = 1e-3\n",
    "\tbeta1 = 0.9\n",
    "\tbeta2 = 0.999\n",
    "\tweight_decay = 1e-2\n",
    "\n",
    "args = TransformerTrainingArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mem_seqs = 128\n",
    "mem_seq = t.randint(1, d_vocab, (num_mem_seqs, model.cfg.n_ctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = Path(\"/content/mem_strs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "        self.optimizer = t.optim.AdamW(self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        self.step = 0\n",
    "        self.weighting = t.ones(args.batch_size)\n",
    "        self.weighting[:num_mem_seqs] = (-t.linspace(-2,2,num_mem_seqs)).exp()\n",
    "\n",
    "    def eval(self):\n",
    "        tokens = t.randint(1, d_vocab, (args.batch_size, model.cfg.n_ctx))\n",
    "        starting_pos = t.randint(1, model.cfg.n_ctx, (1,)).item()\n",
    "        tokens[:num_mem_seqs, starting_pos:] = mem_seq[:,:-starting_pos]\n",
    "        tokens[:,0] = 0\n",
    "        logits = model(tokens)\n",
    "        log_probs = logits.log_softmax(dim=-1)\n",
    "        loss = (-log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1) * self.weighting[:, None]).mean()\n",
    "        return starting_pos, log_probs, loss\n",
    "\n",
    "    def train(self):\n",
    "        try:\n",
    "            for i in tqdm(range(self.args.num_steps)):\n",
    "                _, _, loss = self.eval()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "        finally:\n",
    "            t.save(model.state_dict(), SAVE_DIR/(\"1l_learned_embed.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "trainer = Trainer(args, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
